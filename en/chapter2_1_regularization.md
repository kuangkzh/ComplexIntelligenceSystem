# Building Intelligence â€” Regularization: Narrowing Sampling Space

**This page is translated by ChatGPT.**

In this chapter, we will introduce various methods for building intelligence, primarily the various means commonly used in deep learning. We will not delve into the principles and formula derivations of each algorithm, as these processes have been extensively covered in various research papers and online blogs.

The aim of this chapter is to help readers develop an intuitive understanding, namely that, as mentioned earlier, all intelligence stems from the same principles, follows the same developmental laws, and exhibits the same properties. Therefore, the construction methods effective in one intelligence can be parallelly transferred to another. What we need to do is to clarify the several elements of intelligence generation during construction, namely the system's state space, target distribution, and expected target distribution.

In the introduction to the principles of intelligence in the first chapter, we mentioned two important inspirations from the birth process of intelligence on how we construct intelligence. The first is that when it is impossible to directly solve a proposition, one can exclude incorrect answers through the means of sampling and filtering the contrapositive propositions, thereby completing an approximate solution. The second is to bridge the gap between the tool objective and the actual objective function as much as possible, avoiding distortion in goal transmission. These two points provide us with two directions for intelligent construction techniques.

This section will introduce the first point, namely introducing prior assumptions to the entire state space, reducing the sampling space to lower the sampling cost and accelerate the solution process.


## Regularization Term

The most straightforward approach is to introduce a regularization term loss during model training, sometimes adding L1 or L2 regularization terms to the loss function to constrain model parameters. The L2 regularization term restricts the size of model parameter values, while L1 regularization aims to induce sparsity in parameters while limiting their values. This is a blunt constraint method, assuming that the distribution of parameters will not deviate too far from the origin or that achieving the desired goal does not require parameters that deviate too much. Introducing such deviations would only increase the cost and risk of convergence.

In other intelligent systems with fewer explicit parameters, it is rare to see the use of norm-constrained parameters. However, if we consider the regularization term as a means of forcibly reducing a large solution space through simple assumptions, we can find many similar methods in other systems. It often manifests as a mandatory standard; for example, in corporate recruitment, there are often direct specifications for age, education, and other hard requirements. This is not because there are no talents among job seekers with lower education levels but because selecting from highly educated candidates alone is sufficient to meet the company's needs. Companies are unwilling to incur significant costs in the sampling space of lower education levels, even though there may be talents far superior to highly educated individuals. Hard screening can also mitigate many risks; even if internal employees collude in recruitment and send cheating candidates with qualifications into the company, it is still much better than admitting completely uneducated cheaters.


## Structural Reuse

Structural reuse is prevalent in all forms of intelligence, where each reuse of structure corresponds to a profound understanding of the properties of the state space. Deep learning commonly employs various reused structures: CNN assumes spatial invariance in data and short-range dependencies dominate; RNN assumes sequence data exhibit Markov properties; Transformer assumes sequence data exhibit spatial invariance and dependencies may exist between arbitrary positions. Despite many studies demonstrating the equivalence of CNNs and fully connected layers, and the universal approximation theorem stating that an infinitely wide single-layer network can fit any continuous function, nobody would actually use an extremely wide single-layer network to fit a complex dataset because a much larger dataset is needed to fit a larger state space.

Reused structures effectively decouple the common complexity in a problem and solve it together, which is a widely applicable problem-solving approach. In mathematics, we abstract algebra from arithmetic operations, then separate abstract algebra from algebra itself. This allows us to apply the study of groups, rings, fields, etc., universally to algebraic systems such as quaternions, linear algebra, algebraic topology, and widely apply them in machine learning, graphics, quantum mechanics, and all other fields without the need to invest significant manpower in developing separate mathematical systems for each field.

In large enterprises, it's common to reuse the same organizational structure across different departments. A department typically consists of leaders, human resources personnel, and other regular employees, who are further categorized into various positions such as front-end, development, data, design, product, sales, etc., each with clearly defined job responsibilities. In reality, we know that an employee may well be capable of fulfilling multiple roles, for example, someone who is both a skilled programmer and a proficient salesperson could save the company more manpower costs. However, enabling each individual to fully utilize their talents would require a significant amount of time for employees to explore their capabilities in different areas, and measuring the outcomes of cross-functional work would also be challenging, increasing the risk of incompetence. Therefore, companies would rather abstract multiple positions with similar job content, training each employee to become a more standardized "screw", and designing a matching stable production mode for this set of screws.

In fact, the success of a business model lies in finding a pattern that can be continuously reused, reducing the marginal cost of reuse through methods of decoupling and re-aggregating complexity. For example, chain stores decouple key issues such as procurement, sales, and branding into a reusable pattern. Online platforms decouple the dissemination of network information into multiple modes such as text, chat, images, videos, etc. Once complex problems are solved, it becomes straightforward to generalize the results to universal scenarios to achieve growth and profitability.

The theory presented in this article is also a form of reused structure. It attempts to summarize and generalize the evolutionary laws and properties of intelligence systems that are widely present in social, economic, natural, and artificial domains into a theoretical framework. This allows the same theory to be effective in various fields, and the generalized applications in different fields can be shared through reused parts, continuously improving the explanatory power of this theory. Meanwhile, other application domains can also mutually drive progress. Of course, as of the writing of this article, the theory is still in the training stage within the author's knowledge domain and has not been generalized to unknown areas.


## Chaos Engineering

Chaos Engineering is a term in software engineering that refers to actively creating fault scenarios to discover vulnerable parts of a system in advance, and optimizing the system's resilience based on its performance under fault conditions. We extend this definition to broader system construction, which actually suggests an assumption: that each subcomponent of a system should contribute equally to achieving the final goal and should possess the same resilience, without the occurrence of single-point failures. In deep learning, the most common method is Dropout, randomly setting parameters in the model to zero with a certain probability. Additionally, some researches introduce random noise during training, deliberately introducing noise risk to prevent overfitting of the model.

Modern enterprises requiring employees to take vacations inadvertently increase the resilience of the system, although vacation policies are generally not established for this purpose. In some higher-demanding enterprises, there indeed exists a randomized mandatory paid leave system. This system randomly requires employees to take a longer period of paid leave (usually on a monthly basis) without prior notice. Similar to rotating government officials to different posts every few years, this is to prevent power from being overly concentrated in individual hands, thereby avoiding single-point risks.



## Summary

The characteristic of regularization methods is to introduce strong assumptions into the system's state space, directly reducing a large number of possible system states, thus greatly reducing the construction cost of the system and avoiding generalization risks beyond the assumptions. However, the cost is that each assumption restricts the flexibility of the system, thereby limiting the upper limit of the system's performance. However, without regularization, it may not even be possible to achieve usable system performance. Some attribute the inefficiency of processes in large enterprises after their expansion to the "big company disease." From another perspective, perhaps it is because large enterprises face far more risks than small companies, and small companies with loose constraints and flexible processes are more prone to encounter catastrophic risks during their growth process. The so-called "big company disease" is also a self-limiting protective measure.

The larger the scale of the system, the more difficult it is to find usable solutions in the vast state space, and the more risk points it faces, the more regularization constraints are needed. The performance loss and cost-effectiveness of regularization methods are strongly related to the validity of the assumptions. An ideal assumption should exclude spaces with risks as much as possible while retaining those with potential. This requires making prior judgments based on profound insights into the inherent connections in the system space.

The current assumption of Large Language Models (LLMs) is that we can achieve human-level intelligence solely within the textual domain, with text sequences being the predominant space characterized by long-range dependencies and translational invariance, which can be modeled using Transformer architecture.

With the aid of billions of tokens of textual data, we can birth LLMs with hundreds of billions of parameters, rivaling human-like conversational abilities. 

However, this assumption is not entirely adequate. Firstly, the textual domain does not encompass all aspects of human intelligence. Moreover, the assumptions of translational invariance and long-range dependencies are somewhat weak. Yet, we happen to conveniently acquire vast amounts of cheap data through text channels, enough to find usable solutions through extensive, exhaustive sampling in the space.

It's evident that intelligence trained in this manner tends to be overly cumbersome, with models effectively encoding a vast amount of knowledge within their parameters. 

If we possess strong enough assumptions to strip away this unnecessary complexity, I speculate that merely 100M "hot" parameters, 1B "cold" parameters (loaded on demand), along with a knowledge repository of 100B non-encoded facts, would suffice to completely achieve the average human intelligence level (including conversational, speech, visual, learning, and motor capabilities).

